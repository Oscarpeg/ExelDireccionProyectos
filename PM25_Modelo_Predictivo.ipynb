{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelo Predictivo de Calidad del Aire PM2.5**\n",
    "## *Análisis de Series de Tiempo y Clasificación para Predicción de Niveles de Contaminación*\n",
    "\n",
    "---\n",
    "\n",
    "### **Introducción**\n",
    "\n",
    "La contaminación del aire por partículas finas (PM2.5) representa uno de los principales riesgos para la salud pública a nivel mundial. Las partículas PM2.5 son partículas en suspensión con un diámetro aerodinámico menor a 2.5 micrómetros, capaces de penetrar profundamente en el sistema respiratorio e incluso alcanzar el torrente sanguíneo.\n",
    "\n",
    "La Organización Mundial de la Salud (OMS) establece límites recomendados de exposición a PM2.5:\n",
    "- **Bueno**: 0-35 µg/m³\n",
    "- **Moderado**: 35-75 µg/m³\n",
    "- **No saludable para grupos sensibles**: 75-115 µg/m³\n",
    "- **No saludable**: 115-150 µg/m³\n",
    "- **Muy no saludable**: 150-250 µg/m³\n",
    "- **Peligroso**: >250 µg/m³\n",
    "\n",
    "### **Objetivo del Estudio**\n",
    "\n",
    "Este análisis busca desarrollar un modelo predictivo que permita **clasificar los niveles de PM2.5 para las próximas 24 horas**, utilizando:\n",
    "\n",
    "1. **Análisis de Series de Tiempo**: Explorar patrones temporales, estacionariedad y autocorrelación\n",
    "2. **Modelos ARIMA/ARMA**: Comprender la estructura temporal de los datos\n",
    "3. **Clasificación con Pipelines**: Predecir categorías de calidad del aire\n",
    "4. **Métricas de Evaluación**: Precision, Recall y Accuracy para evaluar el rendimiento\n",
    "\n",
    "### **Los Datos**\n",
    "\n",
    "El dataset utilizado contiene **43,824 observaciones horarias** de calidad del aire y variables meteorológicas, incluyendo:\n",
    "\n",
    "| Variable | Descripción |\n",
    "|----------|-------------|\n",
    "| `pm2.5` | Concentración de PM2.5 (µg/m³) - **Variable objetivo** |\n",
    "| `DEWP` | Punto de rocío (°C) |\n",
    "| `TEMP` | Temperatura (°C) |\n",
    "| `PRES` | Presión atmosférica (hPa) |\n",
    "| `cbwd` | Dirección combinada del viento |\n",
    "| `Iws` | Velocidad del viento acumulada (m/s) |\n",
    "| `Is` | Horas acumuladas de nieve |\n",
    "| `Ir` | Horas acumuladas de lluvia |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **1. Configuración del Entorno e Importación de Librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerías necesarias (si no están instaladas)\n",
    "!pip install statsmodels pmdarima -q\n",
    "\n",
    "# Importación de librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Series de tiempo\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Machine Learning y Pipelines\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Modelos de clasificación\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, classification_report, confusion_matrix,\n",
    "                             precision_recall_curve, roc_auc_score)\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✓ Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **2. Carga y Exploración Inicial de Datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos desde GitHub (reemplazar con tu URL)\n",
    "# Opción 1: Desde GitHub raw\n",
    "url = 'https://raw.githubusercontent.com/Oscarpeg/ExelDireccionProyectos/main/pm25%20(1).csv'\n",
    "\n",
    "# Opción 2: Subir archivo manualmente a Colab\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# df = pd.read_csv('pm25 (1).csv')\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "print(f\"Dataset cargado: {df.shape[0]:,} filas × {df.shape[1]} columnas\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información general del dataset\n",
    "print(\"=\"*60)\n",
    "print(\"INFORMACIÓN GENERAL DEL DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDimensiones: {df.shape[0]:,} registros × {df.shape[1]} variables\")\n",
    "print(f\"\\nPeríodo temporal: {df['year'].min()} - {df['year'].max()}\")\n",
    "print(f\"\\nVariables:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  • {col}: {df[col].dtype}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALORES FALTANTES\")\n",
    "print(\"=\"*60)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Faltantes': missing, 'Porcentaje (%)': missing_pct})\n",
    "print(missing_df[missing_df['Faltantes'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas descriptivas\n",
    "print(\"=\"*60)\n",
    "print(\"ESTADÍSTICAS DESCRIPTIVAS\")\n",
    "print(\"=\"*60)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **3. Preprocesamiento de Datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear columna de fecha/hora\n",
    "df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']])\n",
    "df = df.set_index('datetime')\n",
    "\n",
    "# Eliminar columnas redundantes\n",
    "df = df.drop(['No', 'year', 'month', 'day', 'hour'], axis=1)\n",
    "\n",
    "print(f\"Rango temporal: {df.index.min()} a {df.index.max()}\")\n",
    "print(f\"\\nNuevas dimensiones: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manejo de valores faltantes en pm2.5\n",
    "print(f\"Valores NA en pm2.5 antes: {df['pm2.5'].isna().sum():,}\")\n",
    "\n",
    "# Interpolación para series de tiempo (mejor que eliminar)\n",
    "df['pm2.5'] = df['pm2.5'].interpolate(method='time')\n",
    "\n",
    "# Para valores restantes al inicio, usar forward fill\n",
    "df['pm2.5'] = df['pm2.5'].fillna(method='bfill')\n",
    "\n",
    "print(f\"Valores NA en pm2.5 después: {df['pm2.5'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear variable objetivo categórica para CLASIFICACIÓN\n",
    "# Basado en estándares de calidad del aire\n",
    "\n",
    "def clasificar_pm25(valor):\n",
    "    \"\"\"Clasifica el nivel de PM2.5 según estándares de calidad del aire\"\"\"\n",
    "    if valor <= 35:\n",
    "        return 'Bueno'\n",
    "    elif valor <= 75:\n",
    "        return 'Moderado'\n",
    "    elif valor <= 150:\n",
    "        return 'No_Saludable'\n",
    "    else:\n",
    "        return 'Peligroso'\n",
    "\n",
    "df['pm25_categoria'] = df['pm2.5'].apply(clasificar_pm25)\n",
    "\n",
    "# Ver distribución de categorías\n",
    "print(\"=\"*60)\n",
    "print(\"DISTRIBUCIÓN DE CATEGORÍAS DE CALIDAD DEL AIRE\")\n",
    "print(\"=\"*60)\n",
    "categoria_counts = df['pm25_categoria'].value_counts()\n",
    "categoria_pct = (categoria_counts / len(df) * 100).round(2)\n",
    "\n",
    "for cat in ['Bueno', 'Moderado', 'No_Saludable', 'Peligroso']:\n",
    "    if cat in categoria_counts.index:\n",
    "        print(f\"  {cat:15} : {categoria_counts[cat]:,} ({categoria_pct[cat]}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de distribución de categorías\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gráfico de barras\n",
    "colors = ['#2ecc71', '#f39c12', '#e74c3c', '#8e44ad']\n",
    "orden = ['Bueno', 'Moderado', 'No_Saludable', 'Peligroso']\n",
    "categoria_ordenada = df['pm25_categoria'].value_counts().reindex(orden)\n",
    "\n",
    "axes[0].bar(categoria_ordenada.index, categoria_ordenada.values, color=colors)\n",
    "axes[0].set_title('Distribución de Categorías de Calidad del Aire', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Categoría')\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Gráfico de pastel\n",
    "axes[1].pie(categoria_ordenada.values, labels=categoria_ordenada.index, autopct='%1.1f%%', \n",
    "            colors=colors, explode=[0.02]*len(categoria_ordenada))\n",
    "axes[1].set_title('Proporción de Categorías', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **4. Análisis de Series de Tiempo**\n",
    "\n",
    "### 4.1 Visualización de la Serie Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar la serie de tiempo completa\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Serie completa\n",
    "axes[0].plot(df.index, df['pm2.5'], linewidth=0.5, alpha=0.7, color='steelblue')\n",
    "axes[0].set_title('Serie Temporal Completa de PM2.5', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Fecha')\n",
    "axes[0].set_ylabel('PM2.5 (µg/m³)')\n",
    "axes[0].axhline(y=35, color='green', linestyle='--', label='Límite Bueno (35)')\n",
    "axes[0].axhline(y=75, color='orange', linestyle='--', label='Límite Moderado (75)')\n",
    "axes[0].axhline(y=150, color='red', linestyle='--', label='Límite No Saludable (150)')\n",
    "axes[0].legend(loc='upper right')\n",
    "\n",
    "# Muestra de un mes (para ver patrones diarios)\n",
    "muestra = df['pm2.5']['2013-06':'2013-06']\n",
    "axes[1].plot(muestra.index, muestra.values, linewidth=1, color='steelblue', marker='o', markersize=2)\n",
    "axes[1].set_title('Detalle: Junio 2013 (Patrones Diarios)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Fecha')\n",
    "axes[1].set_ylabel('PM2.5 (µg/m³)')\n",
    "axes[1].axhline(y=35, color='green', linestyle='--', alpha=0.7)\n",
    "axes[1].axhline(y=75, color='orange', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Análisis de Estacionariedad\n",
    "\n",
    "Una serie es **estacionaria** si sus propiedades estadísticas (media, varianza, covarianza) no cambian con el tiempo. Esto es fundamental para modelos ARMA/ARIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_estacionariedad(serie, nombre='Serie'):\n",
    "    \"\"\"\n",
    "    Realiza el test de Dickey-Fuller Aumentado (ADF) para verificar estacionariedad.\n",
    "    \n",
    "    Hipótesis:\n",
    "    - H0: La serie tiene raíz unitaria (NO es estacionaria)\n",
    "    - H1: La serie NO tiene raíz unitaria (ES estacionaria)\n",
    "    \n",
    "    Si p-valor < 0.05, rechazamos H0 → Serie ES ESTACIONARIA\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"TEST DE DICKEY-FULLER AUMENTADO (ADF) - {nombre}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    resultado = adfuller(serie.dropna(), autolag='AIC')\n",
    "    \n",
    "    print(f'Estadístico ADF: {resultado[0]:.4f}')\n",
    "    print(f'P-valor: {resultado[1]:.6f}')\n",
    "    print(f'Lags utilizados: {resultado[2]}')\n",
    "    print(f'Observaciones: {resultado[3]}')\n",
    "    print('\\nValores críticos:')\n",
    "    for key, value in resultado[4].items():\n",
    "        print(f'   {key}: {value:.4f}')\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    if resultado[1] < 0.05:\n",
    "        print(\"✓ CONCLUSIÓN: La serie ES ESTACIONARIA (p-valor < 0.05)\")\n",
    "        print(\"  Se rechaza H0: la serie no tiene raíz unitaria\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"✗ CONCLUSIÓN: La serie NO ES ESTACIONARIA (p-valor >= 0.05)\")\n",
    "        print(\"  No se puede rechazar H0: la serie podría tener raíz unitaria\")\n",
    "        return False\n",
    "\n",
    "# Test en la serie original\n",
    "es_estacionaria = test_estacionariedad(df['pm2.5'], 'PM2.5 Original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si no es estacionaria, aplicar diferenciación\n",
    "if not es_estacionaria:\n",
    "    print(\"\\nAplicando diferenciación de primer orden...\\n\")\n",
    "    df['pm25_diff'] = df['pm2.5'].diff()\n",
    "    test_estacionariedad(df['pm25_diff'].dropna(), 'PM2.5 Diferenciada (d=1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Análisis de Covarianza y Autocorrelación\n",
    "\n",
    "La **covarianza** en series de tiempo mide cómo los valores de la serie en diferentes momentos se relacionan entre sí.\n",
    "\n",
    "- **ACF (Autocorrelación)**: Correlación entre la serie y sus valores rezagados\n",
    "- **PACF (Autocorrelación Parcial)**: Correlación directa entre observaciones separadas por k períodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular y mostrar covarianza entre PM2.5 y sus lags\n",
    "print(\"=\"*60)\n",
    "print(\"ANÁLISIS DE COVARIANZA TEMPORAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Crear lags\n",
    "lags_covarianza = [1, 6, 12, 24, 48, 168]  # 1h, 6h, 12h, 1día, 2días, 1semana\n",
    "print(\"\\nCovarianza entre PM2.5(t) y PM2.5(t-k):\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "for lag in lags_covarianza:\n",
    "    cov = df['pm2.5'].cov(df['pm2.5'].shift(lag))\n",
    "    corr = df['pm2.5'].corr(df['pm2.5'].shift(lag))\n",
    "    if lag == 1:\n",
    "        lag_desc = f\"{lag} hora\"\n",
    "    elif lag < 24:\n",
    "        lag_desc = f\"{lag} horas\"\n",
    "    elif lag == 24:\n",
    "        lag_desc = \"1 día (24h)\"\n",
    "    elif lag == 48:\n",
    "        lag_desc = \"2 días (48h)\"\n",
    "    else:\n",
    "        lag_desc = \"1 semana (168h)\"\n",
    "    print(f\"  Lag {lag:3d} ({lag_desc:12}): Cov = {cov:12.2f}, Corr = {corr:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"INTERPRETACIÓN:\")\n",
    "print(\"• Covarianza alta → valores similares tienden a ocurrir juntos\")\n",
    "print(\"• Correlación cercana a 1 → fuerte dependencia temporal\")\n",
    "print(\"• Decaimiento lento → memoria larga en la serie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficos ACF y PACF\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# ACF serie original (48 lags = 2 días)\n",
    "plot_acf(df['pm2.5'].dropna(), lags=48, ax=axes[0, 0], alpha=0.05)\n",
    "axes[0, 0].set_title('ACF - Serie Original (48 horas)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Lag (horas)')\n",
    "\n",
    "# PACF serie original\n",
    "plot_pacf(df['pm2.5'].dropna(), lags=48, ax=axes[0, 1], alpha=0.05)\n",
    "axes[0, 1].set_title('PACF - Serie Original (48 horas)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Lag (horas)')\n",
    "\n",
    "# ACF serie diferenciada\n",
    "if 'pm25_diff' in df.columns:\n",
    "    plot_acf(df['pm25_diff'].dropna(), lags=48, ax=axes[1, 0], alpha=0.05)\n",
    "    axes[1, 0].set_title('ACF - Serie Diferenciada (48 horas)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Lag (horas)')\n",
    "    \n",
    "    plot_pacf(df['pm25_diff'].dropna(), lags=48, ax=axes[1, 1], alpha=0.05)\n",
    "    axes[1, 1].set_title('PACF - Serie Diferenciada (48 horas)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Lag (horas)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INTERPRETACIÓN DE ACF Y PACF\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "• ACF (Autocorrelación): \n",
    "  - Decaimiento lento → indica componente AR\n",
    "  - Corte abrupto en lag q → indica orden MA(q)\n",
    "  \n",
    "• PACF (Autocorrelación Parcial):\n",
    "  - Corte abrupto en lag p → indica orden AR(p)\n",
    "  - Decaimiento lento → indica componente MA\n",
    "  \n",
    "• Para ARMA(p,q): Ambos decaen gradualmente\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Descomposición de la Serie Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomposición de la serie (usando datos diarios para mejor visualización)\n",
    "pm25_diario = df['pm2.5'].resample('D').mean()\n",
    "\n",
    "# Descomposición aditiva\n",
    "descomposicion = seasonal_decompose(pm25_diario.dropna(), model='additive', period=365)\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "\n",
    "descomposicion.observed.plot(ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Serie Observada', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('PM2.5')\n",
    "\n",
    "descomposicion.trend.plot(ax=axes[1], color='orange')\n",
    "axes[1].set_title('Tendencia', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('PM2.5')\n",
    "\n",
    "descomposicion.seasonal.plot(ax=axes[2], color='green')\n",
    "axes[2].set_title('Estacionalidad (Anual)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('PM2.5')\n",
    "\n",
    "descomposicion.resid.plot(ax=axes[3], color='red')\n",
    "axes[3].set_title('Residuos', fontsize=12, fontweight='bold')\n",
    "axes[3].set_ylabel('PM2.5')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCOMPONENTES DE LA SERIE:\")\n",
    "print(\"• Tendencia: Movimiento a largo plazo\")\n",
    "print(\"• Estacionalidad: Patrones que se repiten (anual)\")\n",
    "print(\"• Residuos: Variación aleatoria no explicada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Modelo ARIMA/ARMA\n",
    "\n",
    "**Conceptos clave:**\n",
    "- **AR (Autoregresivo)**: El valor actual depende de valores pasados\n",
    "  - $y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\epsilon_t$\n",
    "  \n",
    "- **MA (Media Móvil)**: El valor actual depende de errores pasados\n",
    "  - $y_t = c + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + ...$\n",
    "  \n",
    "- **ARMA**: Combinación de AR y MA\n",
    "- **ARIMA**: ARMA con diferenciación para series no estacionarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar una muestra para el modelo ARIMA (datos horarios del último año)\n",
    "pm25_muestra = df['pm2.5']['2014-01-01':].copy()\n",
    "\n",
    "print(f\"Usando {len(pm25_muestra):,} observaciones para el modelo ARIMA\")\n",
    "print(f\"Período: {pm25_muestra.index.min()} a {pm25_muestra.index.max()}\")\n",
    "\n",
    "# Auto ARIMA para encontrar los mejores parámetros\n",
    "print(\"\\nBuscando los mejores parámetros ARIMA (esto puede tomar unos minutos)...\")\n",
    "\n",
    "modelo_auto = auto_arima(\n",
    "    pm25_muestra,\n",
    "    start_p=0, start_q=0,\n",
    "    max_p=3, max_q=3,\n",
    "    d=None,  # Auto-detectar diferenciación\n",
    "    seasonal=False,\n",
    "    trace=True,\n",
    "    error_action='ignore',\n",
    "    suppress_warnings=True,\n",
    "    stepwise=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MEJOR MODELO ENCONTRADO\")\n",
    "print(\"=\"*60)\n",
    "print(modelo_auto.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción con ARIMA para las próximas 24 horas\n",
    "n_prediccion = 24\n",
    "prediccion_arima, conf_int = modelo_auto.predict(n_periods=n_prediccion, return_conf_int=True)\n",
    "\n",
    "# Crear índice para predicciones\n",
    "ultimo_indice = pm25_muestra.index[-1]\n",
    "indice_prediccion = pd.date_range(start=ultimo_indice + pd.Timedelta(hours=1), periods=n_prediccion, freq='H')\n",
    "\n",
    "# Visualizar\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# Últimas 72 horas de datos reales\n",
    "ultimas_horas = pm25_muestra[-72:]\n",
    "ax.plot(ultimas_horas.index, ultimas_horas.values, 'b-', label='Datos Reales', linewidth=2)\n",
    "\n",
    "# Predicciones\n",
    "ax.plot(indice_prediccion, prediccion_arima, 'r--', label='Predicción ARIMA (24h)', linewidth=2)\n",
    "ax.fill_between(indice_prediccion, conf_int[:, 0], conf_int[:, 1], color='red', alpha=0.2, label='Intervalo de Confianza 95%')\n",
    "\n",
    "ax.axhline(y=35, color='green', linestyle=':', alpha=0.7, label='Límite Bueno')\n",
    "ax.axhline(y=75, color='orange', linestyle=':', alpha=0.7, label='Límite Moderado')\n",
    "\n",
    "ax.set_title('Predicción ARIMA de PM2.5 para las Próximas 24 Horas', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Fecha/Hora')\n",
    "ax.set_ylabel('PM2.5 (µg/m³)')\n",
    "ax.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar predicciones\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICCIONES ARIMA PARA LAS PRÓXIMAS 24 HORAS\")\n",
    "print(\"=\"*60)\n",
    "for i, (fecha, pred) in enumerate(zip(indice_prediccion, prediccion_arima)):\n",
    "    categoria = clasificar_pm25(pred)\n",
    "    print(f\"Hora {i+1:2d} ({fecha.strftime('%Y-%m-%d %H:%M')}): {pred:6.1f} µg/m³ → {categoria}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **5. Modelo de Clasificación con Pipelines**\n",
    "\n",
    "Ahora construiremos un modelo de **clasificación** para predecir la categoría de calidad del aire (Bueno, Moderado, No_Saludable, Peligroso) utilizando **Pipelines** de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para clasificación\n",
    "df_clasif = df.copy()\n",
    "\n",
    "# Feature Engineering: agregar variables temporales y lags\n",
    "df_clasif['hora'] = df_clasif.index.hour\n",
    "df_clasif['dia_semana'] = df_clasif.index.dayofweek\n",
    "df_clasif['mes'] = df_clasif.index.month\n",
    "df_clasif['es_fin_semana'] = (df_clasif['dia_semana'] >= 5).astype(int)\n",
    "\n",
    "# Lags de PM2.5 (valores pasados)\n",
    "for lag in [1, 2, 3, 6, 12, 24]:\n",
    "    df_clasif[f'pm25_lag_{lag}'] = df_clasif['pm2.5'].shift(lag)\n",
    "\n",
    "# Media móvil\n",
    "df_clasif['pm25_media_6h'] = df_clasif['pm2.5'].rolling(window=6).mean()\n",
    "df_clasif['pm25_media_24h'] = df_clasif['pm2.5'].rolling(window=24).mean()\n",
    "\n",
    "# Eliminar filas con NaN\n",
    "df_clasif = df_clasif.dropna()\n",
    "\n",
    "print(f\"Datos para clasificación: {df_clasif.shape[0]:,} registros\")\n",
    "df_clasif.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear variable objetivo para predicción a 24 horas\n",
    "# Queremos predecir la categoría de PM2.5 en 24 horas\n",
    "df_clasif['target_24h'] = df_clasif['pm25_categoria'].shift(-24)\n",
    "df_clasif = df_clasif.dropna()\n",
    "\n",
    "# Definir features y target\n",
    "features_numericas = ['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir',\n",
    "                      'hora', 'dia_semana', 'mes', 'es_fin_semana',\n",
    "                      'pm25_lag_1', 'pm25_lag_2', 'pm25_lag_3', \n",
    "                      'pm25_lag_6', 'pm25_lag_12', 'pm25_lag_24',\n",
    "                      'pm25_media_6h', 'pm25_media_24h', 'pm2.5']\n",
    "\n",
    "features_categoricas = ['cbwd']\n",
    "\n",
    "X = df_clasif[features_numericas + features_categoricas]\n",
    "y = df_clasif['target_24h']\n",
    "\n",
    "# Codificar target\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Muestras: {X.shape[0]:,}\")\n",
    "print(f\"\\nClases: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# División temporal de datos (importante para series de tiempo)\n",
    "# No usar split aleatorio - mantener orden temporal\n",
    "\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y_encoded[:split_idx], y_encoded[split_idx:]\n",
    "\n",
    "print(f\"Conjunto de entrenamiento: {len(X_train):,} muestras\")\n",
    "print(f\"Conjunto de prueba: {len(X_test):,} muestras\")\n",
    "print(f\"\\nPeríodo entrenamiento: {X_train.index.min()} a {X_train.index.max()}\")\n",
    "print(f\"Período prueba: {X_test.index.min()} a {X_test.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear Pipeline de preprocesamiento\n",
    "\n",
    "# Preprocesador para variables numéricas\n",
    "preprocessor_numerico = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocesador para variables categóricas\n",
    "preprocessor_categorico = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combinar preprocesadores\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', preprocessor_numerico, features_numericas),\n",
    "        ('cat', preprocessor_categorico, features_categoricas)\n",
    "    ])\n",
    "\n",
    "print(\"✓ Pipeline de preprocesamiento creado\")\n",
    "print(\"\\nEstructura del preprocesador:\")\n",
    "print(\"  1. Variables numéricas: Imputación (mediana) → Escalado (StandardScaler)\")\n",
    "print(\"  2. Variables categóricas: Imputación (moda) → One-Hot Encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear Pipelines completos con diferentes modelos\n",
    "\n",
    "modelos = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "pipelines = {}\n",
    "for nombre, modelo in modelos.items():\n",
    "    pipelines[nombre] = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', modelo)\n",
    "    ])\n",
    "\n",
    "print(\"✓ Pipelines creados para los siguientes modelos:\")\n",
    "for nombre in pipelines.keys():\n",
    "    print(f\"  • {nombre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar y evaluar modelos\n",
    "resultados = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ENTRENAMIENTO Y EVALUACIÓN DE MODELOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for nombre, pipeline in pipelines.items():\n",
    "    print(f\"\\nEntrenando {nombre}...\")\n",
    "    \n",
    "    # Entrenar\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predecir\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    resultados.append({\n",
    "        'Modelo': nombre,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "# Tabla de resultados\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "df_resultados = df_resultados.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARACIÓN DE MODELOS\")\n",
    "print(\"=\"*80)\n",
    "print(df_resultados.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **6. Evaluación Detallada del Mejor Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar el mejor modelo\n",
    "mejor_modelo_nombre = df_resultados.iloc[0]['Modelo']\n",
    "mejor_pipeline = pipelines[mejor_modelo_nombre]\n",
    "\n",
    "print(f\"Mejor modelo: {mejor_modelo_nombre}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Predicciones del mejor modelo\n",
    "y_pred_mejor = mejor_pipeline.predict(X_test)\n",
    "\n",
    "# Reporte de clasificación detallado\n",
    "print(\"\\nREPORTE DE CLASIFICACIÓN DETALLADO\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred_mejor, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred_mejor)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Matriz de confusión absoluta\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=le.classes_, yticklabels=le.classes_, ax=axes[0])\n",
    "axes[0].set_title(f'Matriz de Confusión - {mejor_modelo_nombre}', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicción')\n",
    "axes[0].set_ylabel('Valor Real')\n",
    "\n",
    "# Matriz de confusión normalizada\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues', \n",
    "            xticklabels=le.classes_, yticklabels=le.classes_, ax=axes[1])\n",
    "axes[1].set_title('Matriz de Confusión Normalizada', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicción')\n",
    "axes[1].set_ylabel('Valor Real')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicación de métricas\n",
    "print(\"=\"*70)\n",
    "print(\"EXPLICACIÓN DE MÉTRICAS DE CLASIFICACIÓN\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│ ACCURACY (Exactitud)                                                │\n",
    "│ ─────────────────────                                               │\n",
    "│ Proporción de predicciones correctas sobre el total.                │\n",
    "│ Fórmula: (VP + VN) / (VP + VN + FP + FN)                           │\n",
    "│                                                                     │\n",
    "│ Interpretación: ¿Qué tan frecuentemente el modelo acierta?         │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│ PRECISION (Precisión)                                               │\n",
    "│ ────────────────────                                                │\n",
    "│ De todas las predicciones positivas, ¿cuántas son correctas?       │\n",
    "│ Fórmula: VP / (VP + FP)                                            │\n",
    "│                                                                     │\n",
    "│ Interpretación: ¿Qué tan confiables son las alertas del modelo?    │\n",
    "│ Alta precisión = pocas falsas alarmas                              │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│ RECALL (Sensibilidad/Exhaustividad)                                 │\n",
    "│ ──────────────────────────────────                                  │\n",
    "│ De todos los casos positivos reales, ¿cuántos detectamos?          │\n",
    "│ Fórmula: VP / (VP + FN)                                            │\n",
    "│                                                                     │\n",
    "│ Interpretación: ¿Qué tan bueno es detectando casos peligrosos?     │\n",
    "│ Alto recall = pocos casos peligrosos sin detectar                  │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│ F1-SCORE                                                            │\n",
    "│ ────────                                                            │\n",
    "│ Media armónica de Precision y Recall.                              │\n",
    "│ Fórmula: 2 × (Precision × Recall) / (Precision + Recall)           │\n",
    "│                                                                     │\n",
    "│ Interpretación: Balance entre precisión y exhaustividad            │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "En el contexto de calidad del aire:\n",
    "• Alta PRECISION → Menos falsas alarmas de contaminación\n",
    "• Alto RECALL → Detectamos más episodios de contaminación real\n",
    "• Para salud pública, RECALL es más importante (no queremos perder\n",
    "  episodios peligrosos)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización comparativa de métricas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gráfico de barras comparativo\n",
    "metricas = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(metricas))\n",
    "width = 0.2\n",
    "\n",
    "for i, (_, row) in enumerate(df_resultados.iterrows()):\n",
    "    valores = [row['Accuracy'], row['Precision'], row['Recall'], row['F1-Score']]\n",
    "    axes[0].bar(x + i*width, valores, width, label=row['Modelo'])\n",
    "\n",
    "axes[0].set_xlabel('Métrica')\n",
    "axes[0].set_ylabel('Valor')\n",
    "axes[0].set_title('Comparación de Métricas por Modelo', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xticks(x + width * 1.5)\n",
    "axes[0].set_xticklabels(metricas)\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Gráfico radar del mejor modelo\n",
    "valores_mejor = df_resultados.iloc[0][['Accuracy', 'Precision', 'Recall', 'F1-Score']].values\n",
    "categorias = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "# Cerrar el gráfico radar\n",
    "valores_mejor = np.concatenate((valores_mejor, [valores_mejor[0]]))\n",
    "angulos = np.linspace(0, 2 * np.pi, len(categorias), endpoint=False).tolist()\n",
    "angulos += angulos[:1]\n",
    "\n",
    "ax_radar = fig.add_subplot(122, polar=True)\n",
    "ax_radar.plot(angulos, valores_mejor, 'o-', linewidth=2, color='steelblue')\n",
    "ax_radar.fill(angulos, valores_mejor, alpha=0.25, color='steelblue')\n",
    "ax_radar.set_xticks(angulos[:-1])\n",
    "ax_radar.set_xticklabels(categorias)\n",
    "ax_radar.set_title(f'Perfil de Métricas - {mejor_modelo_nombre}', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Ocultar el segundo subplot original\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **7. Validación con Series de Tiempo (TimeSeriesSplit)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validación cruzada específica para series de tiempo\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDACIÓN CRUZADA CON TIMESERIESSPLIT\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nEste método respeta el orden temporal de los datos,\")\n",
    "print(\"entrenando siempre con datos pasados y validando con futuros.\\n\")\n",
    "\n",
    "# Preparar datos (sin índice de tiempo)\n",
    "X_array = preprocessor.fit_transform(X)\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    scores = cross_val_score(modelo, X_array, y_encoded, cv=tscv, scoring='accuracy')\n",
    "    print(f\"{nombre:20}: Accuracy = {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar los folds de TimeSeriesSplit\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    ax.scatter(train_idx, [i+1]*len(train_idx), c='blue', marker='s', s=1, label='Train' if i==0 else '')\n",
    "    ax.scatter(test_idx, [i+1]*len(test_idx), c='red', marker='s', s=1, label='Test' if i==0 else '')\n",
    "\n",
    "ax.set_xlabel('Índice de muestra (tiempo →)')\n",
    "ax.set_ylabel('Fold')\n",
    "ax.set_title('Visualización de TimeSeriesSplit (5 folds)', fontsize=12, fontweight='bold')\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_yticks([1, 2, 3, 4, 5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNota: El conjunto de entrenamiento (azul) siempre precede al de prueba (rojo)\")\n",
    "print(\"      Esto simula predicción de eventos futuros basada en datos pasados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **8. Predicción de las Próximas 24 Horas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las últimas observaciones para predecir\n",
    "ultimas_obs = X.iloc[-24:].copy()\n",
    "\n",
    "# Predecir con el mejor modelo\n",
    "predicciones_24h = mejor_pipeline.predict(ultimas_obs)\n",
    "predicciones_labels = le.inverse_transform(predicciones_24h)\n",
    "\n",
    "# Si el modelo soporta probabilidades\n",
    "if hasattr(mejor_pipeline.named_steps['classifier'], 'predict_proba'):\n",
    "    probabilidades = mejor_pipeline.predict_proba(ultimas_obs)\n",
    "else:\n",
    "    probabilidades = None\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PREDICCIÓN DE CALIDAD DEL AIRE - PRÓXIMAS 24 HORAS\")\n",
    "print(f\"Modelo utilizado: {mejor_modelo_nombre}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear DataFrame con predicciones\n",
    "df_pred = pd.DataFrame({\n",
    "    'Hora': range(1, 25),\n",
    "    'Fecha_Hora': pd.date_range(start=ultimas_obs.index[-1] + pd.Timedelta(hours=1), periods=24, freq='H'),\n",
    "    'Categoría_Predicha': predicciones_labels\n",
    "})\n",
    "\n",
    "if probabilidades is not None:\n",
    "    for i, clase in enumerate(le.classes_):\n",
    "        df_pred[f'Prob_{clase}'] = probabilidades[:, i]\n",
    "\n",
    "print(df_pred.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de predicciones\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Colores por categoría\n",
    "color_map = {'Bueno': '#2ecc71', 'Moderado': '#f39c12', 'No_Saludable': '#e74c3c', 'Peligroso': '#8e44ad'}\n",
    "colores = [color_map.get(cat, 'gray') for cat in predicciones_labels]\n",
    "\n",
    "# Gráfico de categorías predichas\n",
    "axes[0].bar(range(1, 25), [1]*24, color=colores, edgecolor='black', linewidth=0.5)\n",
    "axes[0].set_xlabel('Hora de predicción')\n",
    "axes[0].set_title('Predicción de Categoría de Calidad del Aire - Próximas 24 Horas', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(range(1, 25))\n",
    "axes[0].set_yticks([])\n",
    "\n",
    "# Leyenda\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=color, label=cat) for cat, color in color_map.items()]\n",
    "axes[0].legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "# Añadir texto con la categoría\n",
    "for i, cat in enumerate(predicciones_labels):\n",
    "    axes[0].text(i+1, 0.5, cat[:3], ha='center', va='center', fontsize=8, fontweight='bold', color='white')\n",
    "\n",
    "# Gráfico de probabilidades (si están disponibles)\n",
    "if probabilidades is not None:\n",
    "    x = range(1, 25)\n",
    "    bottom = np.zeros(24)\n",
    "    for i, clase in enumerate(le.classes_):\n",
    "        axes[1].bar(x, probabilidades[:, i], bottom=bottom, label=clase, \n",
    "                   color=color_map.get(clase, 'gray'), alpha=0.8)\n",
    "        bottom += probabilidades[:, i]\n",
    "    \n",
    "    axes[1].set_xlabel('Hora de predicción')\n",
    "    axes[1].set_ylabel('Probabilidad')\n",
    "    axes[1].set_title('Distribución de Probabilidades por Hora', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xticks(range(1, 25))\n",
    "    axes[1].legend(loc='upper right')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen de predicciones\n",
    "print(\"=\"*60)\n",
    "print(\"RESUMEN DE PREDICCIONES - PRÓXIMAS 24 HORAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "resumen = pd.Series(predicciones_labels).value_counts()\n",
    "for cat in ['Bueno', 'Moderado', 'No_Saludable', 'Peligroso']:\n",
    "    if cat in resumen.index:\n",
    "        horas = resumen[cat]\n",
    "        pct = horas / 24 * 100\n",
    "        print(f\"  {cat:15}: {horas:2d} horas ({pct:.1f}%)\")\n",
    "\n",
    "# Alertas\n",
    "horas_peligrosas = sum(1 for cat in predicciones_labels if cat in ['No_Saludable', 'Peligroso'])\n",
    "if horas_peligrosas > 0:\n",
    "    print(\"\\n\" + \"⚠\"*30)\n",
    "    print(f\"  ALERTA: Se predicen {horas_peligrosas} horas con calidad del aire\")\n",
    "    print(f\"          no saludable o peligrosa en las próximas 24 horas.\")\n",
    "    print(\"⚠\"*30)\n",
    "else:\n",
    "    print(\"\\n✓ No se predicen episodios de contaminación severa en las próximas 24 horas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **9. Importancia de Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener importancia de variables (para modelos basados en árboles)\n",
    "if mejor_modelo_nombre in ['Random Forest', 'Gradient Boosting']:\n",
    "    # Obtener nombres de features después del preprocesamiento\n",
    "    feature_names = features_numericas.copy()\n",
    "    \n",
    "    # Añadir nombres de features categóricas (one-hot encoded)\n",
    "    if hasattr(preprocessor.named_transformers_['cat'].named_steps['onehot'], 'get_feature_names_out'):\n",
    "        cat_features = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(features_categoricas)\n",
    "        feature_names.extend(cat_features)\n",
    "    \n",
    "    importancia = mejor_pipeline.named_steps['classifier'].feature_importances_\n",
    "    \n",
    "    # Crear DataFrame de importancia\n",
    "    df_importancia = pd.DataFrame({\n",
    "        'Feature': feature_names[:len(importancia)],\n",
    "        'Importancia': importancia\n",
    "    }).sort_values('Importancia', ascending=True)\n",
    "    \n",
    "    # Visualizar\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(df_importancia)))\n",
    "    ax.barh(df_importancia['Feature'], df_importancia['Importancia'], color=colors)\n",
    "    ax.set_xlabel('Importancia')\n",
    "    ax.set_title(f'Importancia de Variables - {mejor_modelo_nombre}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 5 variables más importantes:\")\n",
    "    for _, row in df_importancia.tail(5).iloc[::-1].iterrows():\n",
    "        print(f\"  • {row['Feature']}: {row['Importancia']:.4f}\")\n",
    "else:\n",
    "    print(f\"La importancia de variables no está disponible para {mejor_modelo_nombre}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **10. Conclusiones**\n",
    "\n",
    "### Hallazgos Principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CONCLUSIONES DEL ANÁLISIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "1. ANÁLISIS DE SERIES DE TIEMPO\n",
    "   ─────────────────────────────\n",
    "   • La serie de PM2.5 presenta patrones de autocorrelación significativos,\n",
    "     indicando que los valores pasados son útiles para predecir valores futuros.\n",
    "   • Se identificó estacionalidad en los datos, con patrones diarios y anuales.\n",
    "   • La covarianza temporal muestra dependencia fuerte en las primeras horas,\n",
    "     decayendo gradualmente con el tiempo.\n",
    "\n",
    "2. MODELOS ARIMA/ARMA\n",
    "   ──────────────────\n",
    "   • Se utilizó auto_arima para identificar automáticamente los mejores\n",
    "     parámetros (p, d, q) para la serie.\n",
    "   • El modelo ARIMA captura la estructura temporal y permite proyecciones\n",
    "     a corto plazo de valores continuos de PM2.5.\n",
    "\n",
    "3. CLASIFICACIÓN CON PIPELINES\n",
    "   ────────────────────────────\n",
    "   • Se implementaron Pipelines de scikit-learn que integran:\n",
    "     - Preprocesamiento (imputación, escalado, encoding)\n",
    "     - Modelos de clasificación\n",
    "   • El uso de Pipelines garantiza reproducibilidad y evita data leakage.\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "4. RENDIMIENTO DE MODELOS\n",
    "   ──────────────────────\n",
    "   • Mejor modelo: {mejor_modelo_nombre}\n",
    "   • Accuracy:  {df_resultados.iloc[0]['Accuracy']:.4f}\n",
    "   • Precision: {df_resultados.iloc[0]['Precision']:.4f}\n",
    "   • Recall:    {df_resultados.iloc[0]['Recall']:.4f}\n",
    "   • F1-Score:  {df_resultados.iloc[0]['F1-Score']:.4f}\n",
    "\n",
    "5. MÉTRICAS DE EVALUACIÓN\n",
    "   ──────────────────────\n",
    "   • ACCURACY: Proporción total de predicciones correctas\n",
    "   • PRECISION: Confiabilidad de las alertas (evitar falsas alarmas)\n",
    "   • RECALL: Capacidad de detectar todos los casos reales (sensibilidad)\n",
    "   • Para salud pública, alto RECALL es prioritario para no perder\n",
    "     episodios de contaminación peligrosa.\n",
    "\n",
    "6. PREDICCIÓN A 24 HORAS\n",
    "   ─────────────────────\n",
    "   • El modelo permite clasificar la calidad del aire esperada\n",
    "     para cada hora de las próximas 24 horas.\n",
    "   • Las variables más importantes para la predicción son los\n",
    "     valores recientes de PM2.5 (lags) y condiciones meteorológicas.\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FIN DEL ANÁLISIS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Referencias**\n",
    "\n",
    "- OMS (2021). WHO global air quality guidelines.\n",
    "- Hyndman, R.J., & Athanasopoulos, G. (2021). Forecasting: principles and practice.\n",
    "- Scikit-learn documentation: https://scikit-learn.org/\n",
    "- Statsmodels documentation: https://www.statsmodels.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
